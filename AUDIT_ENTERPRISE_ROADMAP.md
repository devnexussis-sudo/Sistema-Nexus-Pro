# ğŸ›ï¸ NEXUS LINE â€” Auditoria Enterprise-Grade (L7 Principal Engineer)

**Data:** 2026-02-18  
**Auditor:** Principal Software Engineer L7  
**Escopo:** Auditoria de 4 Pilares CrÃ­ticos para SaaS Comercial de Alto NÃ­vel  
**Status:** âš ï¸ NÃƒO PRONTO PARA PRODUÃ‡ÃƒO ENTERPRISE â€” Leia abaixo  

---

## ğŸ“‹ SUMÃRIO EXECUTIVO

O Nexus Line possui uma base funcional sÃ³lida, com boas intenÃ§Ãµes arquiteturais (multi-tenancy via RLS, singleton Supabase, sessÃµes isoladas por aba). No entanto, **apresenta 14 inconsistÃªncias fatais e 23 problemas graves** que impedem sua classificaÃ§Ã£o como SaaS comercial Enterprise-Grade. Os problemas mais crÃ­ticos sÃ£o: **bypass total de RLS pelo uso massivo de `adminSupabase`**, **Service Worker desabilitado**, **duplicaÃ§Ã£o catastrÃ³fica da funÃ§Ã£o `getCurrentTenantId`** e **ausÃªncia de gerenciamento de estado offline**.

---

## ğŸ”´ PILAR 1: ARQUITETURA DE RESILIÃŠNCIA (O Bug de Inatividade)

### 1.1 âœ… O que estÃ¡ BEM
- Cliente Supabase instanciado como **singleton** (`supabase.ts` linha 16) â€” correto.
- `autoRefreshToken: true` habilitado â€” correto.
- `ensureValidSession()` com cooldown de 10s para evitar flood.
- Heartbeat do Realtime configurado em 15s com reconnect exponential backoff.
- Custom fetch com timeout de 30s para prevenir requests pendurados.
- AuthContext com listener `onAuthStateChange` para reagir a TOKEN_REFRESHED.
- Inactivity check de 1.5h com cleanup de sessÃ£o.
- Listener `focus` e `online` para restaurar sessÃ£o apÃ³s offline.

### 1.2 ğŸ”´ INCONSISTÃŠNCIAS FATAIS

#### FATAL-R1: Race Condition no Token Refresh (Dupla Chamada)
**Arquivo:** `src/contexts/AuthContext.tsx` linhas 57-70  
**Problema:** O `validateAndRestoreSession()` faz `supabase.auth.refreshSession()` manualmente quando detecta token prÃ³ximo de expirar. PorÃ©m, o `autoRefreshToken: true` do cliente jÃ¡ estÃ¡ fazendo isso automaticamente. O prÃ³prio comentÃ¡rio no `supabase.ts` (linha 71) diz: *"Do NOT manually refresh if autoRefreshToken is on. Manual refresh creates race conditions."* â€” **mas o AuthContext faz exatamente o contrÃ¡rio**.  
**Impacto:** Race condition entre refresh manual e auto-refresh. Em cenÃ¡rios de alta latÃªncia (3G/4G), ambos podem executar simultaneamente, invalidando o token do outro.  
**Severidade:** ğŸ”´ CRÃTICA  
**CorreÃ§Ã£o:** Remover o refresh manual do AuthContext. Confiar exclusivamente no `autoRefreshToken` do SDK. O `validateAndRestoreSession` deve apenas verificar se a sessÃ£o existe, nÃ£o forÃ§ar refresh.

#### FATAL-R2: Session Guard Duplicado e Inconsistente
**Arquivos:** `src/lib/supabase.ts` (linhas 64-90) vs `src/services/orderService.ts` (linhas 43-55)  
**Problema:** Existem DUAS implementaÃ§Ãµes diferentes de `ensureValidSession`:
- `supabase.ts`: Apenas verifica sessÃ£o, **nÃ£o** faz refresh manual (correto).
- `orderService.ts`: Faz `refreshSession()` manualmente (incorreto, conflita com auto-refresh).  
**Impacto:** Comportamento inconsistente. OrderService pode invalidar tokens ativos.  
**Severidade:** ğŸ”´ CRÃTICA  
**CorreÃ§Ã£o:** Usar apenas o `ensureValidSession` de `supabase.ts` em todo o projeto. Deletar a versÃ£o duplicada no `orderService.ts`.

#### FATAL-R3: handleFocus Chama validateAndRestoreSession DUAS Vezes
**Arquivo:** `src/contexts/AuthContext.tsx` linhas 134-143  
**Problema:** A funÃ§Ã£o `handleFocus` chama `validateAndRestoreSession` duas vezes seguidas (linhas 137-138), uma com `silent=false` e outra com `silent=true`. Isso gera o DOBRO de chamadas desnecessÃ¡rias ao Supabase Auth toda vez que o usuÃ¡rio volta Ã  aba.  
**Impacto:** DesperdÃ­cio de requisiÃ§Ãµes, risco de rate limit em cenÃ¡rios com muitas abas.  
**Severidade:** ğŸŸ  ALTA  
**CorreÃ§Ã£o:** Chamar apenas uma vez. A lÃ³gica condicional de `!currentTenant` pode ser resolvida em uma Ãºnica chamada.

#### FATAL-R4: `adminSupabase` Ã‰ Um Objeto HÃ­brido InstÃ¡vel
**Arquivo:** `src/lib/supabase.ts` linhas 222-234  
**Problema:** O `adminSupabase` Ã© criado com spread (`{...supabase, auth: {...}}`) e cast `as any`. Isso **NÃƒO** Ã© um cliente Supabase real â€” Ã© um objeto plain JS que perdeu os mÃ©todos de prototype do SDK. MÃ©todos como `.channel()`, `.realtime`, etc., podem quebrar silenciosamente.  
**Impacto:** Comportamento imprevisÃ­vel. Qualquer atualizaÃ§Ã£o do SDK Supabase pode quebrar completamente.  
**Severidade:** ğŸ”´ CRÃTICA  
**CorreÃ§Ã£o:** NÃ£o usar spread. O `adminSupabase` deve ser o prÃ³prio `supabase` com o admin proxy injetado separadamente. As operaÃ§Ãµes admin devem usar `adminAuthProxy` diretamente, nÃ£o misturado em um objeto Frankenstein.

#### GRAVE-R5: AbortController Leak no Custom Fetch
**Arquivo:** `src/lib/supabase.ts` linhas 24-43  
**Problema:** O `AbortController` criado no custom fetch encadeia com o signal original via `addEventListener('abort', ...)`, mas **nunca remove** esse listener. Em aplicaÃ§Ãµes de longa duraÃ§Ã£o (SPA), isso pode acumular centenas de listeners no signal original do Supabase.  
**Severidade:** ğŸŸ  ALTA  
**CorreÃ§Ã£o:** Usar `AbortSignal.any()` (API moderna) ou garantir cleanup do listener no `.finally()`.

---

## ğŸ”´ PILAR 2: PADRÃ•ES DE ESCALABILIDADE SaaS (Multi-Tenancy & RLS)

### 2.1 âœ… O que estÃ¡ BEM
- RLS habilitado em todas as tabelas core (migrations verificadas).
- FunÃ§Ã£o `get_user_tenant_id()` como SECURITY DEFINER â€” correto.
- Policies usando `get_user_tenant_id()` para isolamento por tenant.
- Fallback de JWT â†’ tabela `users` para resolver tenant â€” robusto.
- Ãndices nos campos `tenant_id` de todas as tabelas â€” bom.
- Audit logs com infraestrutura criada (embora triggers comentados).

### 2.2 ğŸ”´ INCONSISTÃŠNCIAS FATAIS

#### FATAL-S1: ğŸš¨ BYPASS TOTAL DO RLS â€” `adminSupabase` Usado Em Todo Lugar ğŸš¨
**Arquivos:** `orderService.ts`, `tenantService.ts`, `technicianService.ts`, `authService.ts`  
**Problema:** O `getServiceClient()` retorna `adminSupabase` em TODOS os services. O admin proxy redireciona operaÃ§Ãµes auth para Edge Functions (correto), MAS `adminSupabase.from('orders')` usa `supabase.from` â€” ou seja, usa o **cliente anon** para queries normais. PorÃ©m, em `tenantService.ts`, as chamadas usam `adminSupabase.from('tenants')` diretamente, e se esse objeto herdar a service role key (ou se no futuro alguÃ©m adicionar a service key), **TODO o RLS serÃ¡ ignorado**.  

Mais grave: O `tenantService.ts` usa `adminSupabase.from('users')` para buscar dados cross-tenant no painel Master (linhas 289, 320). Se um admin de um tenant conseguir acessar essas funÃ§Ãµes, **pode ver dados de outros tenants**.  
**Impacto:** Risco de VAZAMENTO DE DADOS entre clientes. ViolaÃ§Ã£o de LGPD.  
**Severidade:** ğŸ”´ğŸ”´ BLOQUEANTE  
**CorreÃ§Ã£o:**
1. Remover `adminSupabase` do frontend completamente.
2. OperaÃ§Ãµes admin devem usar Edge Functions (que jÃ¡ existem para auth).
3. Queries normais devem usar o `supabase` client (anon) que respeita RLS.
4. OperaÃ§Ãµes Master devem ser auth-guarded por Edge Functions com verificaÃ§Ã£o de role.

#### FATAL-S2: Tenant ID ExtraÃ­do de URL Params sem ValidaÃ§Ã£o
**Arquivo:** `src/lib/tenantContext.ts` linha 60-64, replicado em ~8 arquivos  
**Problema:** O `getCurrentTenantId()` aceita `tid` de query params da URL (`?tid=xxx`). Um atacante pode adicionar `?tid=UUID_DE_OUTRO_TENANT` Ã  URL e, se qualquer cÃ³digo confiar nesse valor sem validar contra o JWT, acessar dados de outro tenant.  
**Impacto:** EscalaÃ§Ã£o horizontal de acesso. Um usuÃ¡rio pode forjar contexto de outro tenant.  
**Severidade:** ğŸ”´ CRÃTICA  
**CorreÃ§Ã£o:** O tenant ID do URL deve ser APENAS informativo (para link pÃºblico). O tenant real deve SEMPRE vir do JWT claims ou da tabela `users`. Remover fallback para URL params em contextos autenticados.

#### FATAL-S3: `getCurrentTenantId()` Duplicado 8+ Vezes
**Arquivos com cÃ³pia idÃªntica:**
1. `src/lib/tenantContext.ts` (TenantContext.getCurrentTenantId)
2. `src/services/authService.ts` (AuthService.getCurrentTenantId)
3. `src/services/orderService.ts` (getCurrentTenantId local)
4. `src/services/tenantService.ts` (getCurrentTenantId local)
5. `src/services/technicianService.ts` (getCurrentTenantId local)
6. `src/services/storageService.ts` (getCurrentTenantId local)
7. `src/services/dataService.ts` (DataService.getCurrentTenantId)
8. Provavelmente mais em `customerService.ts`, `equipmentService.ts`, etc.  

**Problema:** CÃ³digo duplicado com ~30 linhas idÃªnticas em cada arquivo. Se um fix precisa ser aplicado (como remover o fallback de URL params), precisa ser feito em 8+ lugares.  
**Nota:** O `TenantContext` foi criado exatamente para resolver isso, mas **nenhum service o usa**. O `useTenantContext` hook tem um bug: importa `React` na linha 177, **depois** de ser usado na linha 158.  
**Severidade:** ğŸ”´ CRÃTICA (manutenibilidade + seguranÃ§a)  
**CorreÃ§Ã£o:** Todos os services devem importar e usar `getCurrentTenantId` de `tenantContext.ts`. Deletar todas as cÃ³pias locais.

#### FATAL-S4: `technicians` Tabela Sem VerificaÃ§Ã£o de Tenant no Frontend
**Arquivo:** `technicianService.ts` linhas 308-311  
**Problema:** `updateTechnicianAvatar` faz update sem filtrar por `tenant_id`: `.update({ avatar }).eq('id', userId)`. Se o RLS falhar (e com `adminSupabase` isso Ã© possÃ­vel), um admin pode alterar o avatar de um tÃ©cnico de outro tenant.  
**Severidade:** ğŸŸ  ALTA  
**CorreÃ§Ã£o:** Sempre incluir `.eq('tenant_id', tenantId)` em queries de update/delete.

#### GRAVE-S5: `deleteTenant` Faz Cascade Manual sem Transaction
**Arquivo:** `tenantService.ts` linhas 282-313  
**Problema:** A exclusÃ£o de tenant deleta dados sequencialmente em 10 tabelas. Se o processo falhar no meio (ex: timeout na tabela 5), as tabelas 1-4 jÃ¡ foram limpas mas o tenant ainda existe. Dados Ã³rfÃ£os.  
**Severidade:** ğŸŸ  ALTA  
**CorreÃ§Ã£o:** Usar uma Edge Function com database transaction (`BEGIN/COMMIT/ROLLBACK`), ou configurar `ON DELETE CASCADE` nas foreign keys.

#### GRAVE-S6: Policies do `users_select_policy` Permitem `OR public.is_admin()`
**Arquivo:** `20260209_clean_and_fix_all_rls.sql` linha 72  
**Problema:** A policy de SELECT permite que qualquer admin veja TODOS os users, independente de tenant: `id = auth.uid() OR tenant_id = get_user_tenant_id() OR public.is_admin()`. O `is_admin()` nÃ£o filtra por tenant, entÃ£o um admin do Tenant A pode ver users do Tenant B.  
**Severidade:** ğŸ”´ CRÃTICA  
**CorreÃ§Ã£o:** Mudar para: `id = auth.uid() OR (tenant_id = get_user_tenant_id()) OR (is_admin() AND tenant_id = get_user_tenant_id())`.

#### GRAVE-S7: `users_delete_admin` Policy Sem Filtro de Tenant
**Arquivo:** `20260209_clean_and_fix_all_rls.sql` linha 80-81  
**Problema:** `USING (public.is_admin())` â€” um admin pode deletar users de QUALQUER tenant.  
**Severidade:** ğŸ”´ CRÃTICA  
**CorreÃ§Ã£o:** Adicionar `AND tenant_id = public.get_user_tenant_id()`.

---

## ğŸ”´ PILAR 3: INTEGRIDADE DO APP (PWA para APK)

### 3.1 âœ… O que estÃ¡ BEM
- `manifest.json` presente com Ã­cones e configuraÃ§Ã£o standalone.
- Loading screen com fallback de 10s (safety timeout).
- HashRouter para compatibilidade com hosts estÃ¡ticos.
- Vite configurado com Brotli + Gzip compression.
- Code splitting manual por vendor chunks.

### 3.2 ğŸ”´ INCONSISTÃŠNCIAS FATAIS

#### FATAL-P1: Service Worker DESABILITADO â€” PWA NÃ£o Funciona
**Arquivo:** `public/sw.js` linhas 1-25  
**Problema:** O service worker estÃ¡ **intencionalmente desabilitado**. Ele faz `self.registration.unregister()` no activate. Isso significa:
- âŒ Nenhum cache offline funciona
- âŒ O app nÃ£o instala como PWA real
- âŒ Nenhum background sync
- âŒ Push notifications nÃ£o funcionam via SW
- âŒ Envolvimento via Trusted Web Activity (TWA) para APK serÃ¡ instÃ¡vel  

**Impacto:** O app Ã© apenas um website responsivo, NÃƒO uma PWA. Para APK via TWA/Capacitor, o SW Ã© obrigatÃ³rio.  
**Severidade:** ğŸ”´ğŸ”´ BLOQUEANTE  
**CorreÃ§Ã£o:** Implementar um SW completo com estratÃ©gias de cache:
- Cache-First para assets estÃ¡ticos
- Network-First para API calls
- Offline Fallback page
- Background Sync para mutations pendentes
- Considerar usar Workbox (plugin Vite disponÃ­vel)

#### FATAL-P2: Sem Gerenciamento de Estado Offline
**Problema geral:**
- Nenhum `store` global (Redux/Zustand/Jotai). Estado Ã© gerenciado via:
  - `AuthContext` (Ãºnico Context provider)
  - `useQuery` hook customizado com cache em `localStorage`
  - `SessionStorage` por aba
- O `useQuery` persiste dados em `localStorage` (bom para offline read), mas:
  - âŒ Nenhum mecanismo de **offline mutation queue** (write-ahead log)
  - âŒ Nenhum conflict resolution para sincronizaÃ§Ã£o
  - âŒ Se o app perde rede durante um `createOrder`, a OS Ã© simplesmente perdida
  - âŒ Nenhum UI feedback de "offline mode"  

**Severidade:** ğŸ”´ CRÃTICA para mobile  
**CorreÃ§Ã£o:** Implementar:
1. Mutation Queue com IndexedDB (mais robusto que localStorage)
2. Background Sync via SW
3. UI indicator de status de rede
4. Retry automÃ¡tico com idempotency keys

#### FATAL-P3: `manifest.json` Incompleto para APK
**Arquivo:** `public/manifest.json`  
**Problemas:**
- âŒ Faltam Ã­cones em mÃºltiplos tamanhos (48, 72, 96, 128, 144, 192, 384, 512)
- âŒ Mesmo arquivo `pwa-icon.png` usado para 512 e 192 (deve ser otimizado por tamanho)
- âŒ `"purpose": "any maskable"` deve ser separado em dois entries (melhor compatibilidade)
- âŒ Falta `id` field (PWA spec recomenda para identidade)
- âŒ Falta `categories` e `screenshots` (melhor instalaÃ§Ã£o)
- âŒ `start_url: "/"` mas o app usa HashRouter â€” deveria ser `"/?source=pwa"` ou `"/#/"`.  
**Severidade:** ğŸŸ  ALTA  
**CorreÃ§Ã£o:** Gerar Ã­cones em todos os tamanhos. Separar `any` e `maskable`. Adicionar campos faltantes.

#### GRAVE-P4: `useQuery` Cache Key Collision
**Arquivo:** `src/hooks/useQuery.ts` linha 58  
**Problema:** Cache key em localStorage Ã© `NEXUS_CACHE_${key}`, mas nÃ£o inclui tenant ID. Se dois usuÃ¡rios de tenants diferentes usarem o mesmo browser, dados de um aparecerÃ£o para o outro.  
**Severidade:** ğŸŸ  ALTA  
**CorreÃ§Ã£o:** Prefixar cache key com tenant ID: `NEXUS_CACHE_${tenantId}_${key}`.

#### GRAVE-P5: OrderDetailsModal.tsx com 63KB
**Arquivo:** `src/tech-pwa/OrderDetailsModal.tsx` â€” **63.2 KB**  
**Problema:** Componente monolÃ­tico com provavelmente 1500+ linhas. Para mobile PWA, isso significa:
- Parse time alto em dispositivos low-end
- ImpossÃ­vel de manter
- Tree-shaking ineficaz  
**Severidade:** ğŸŸ  ALTA  
**CorreÃ§Ã£o:** Dividir em sub-componentes: OrderHeader, OrderStatusFlow, OrderForm, OrderItems, OrderSignature, OrderActions, etc.

---

## ğŸ”´ PILAR 4: CLEAN CODE & DÃVIDA TÃ‰CNICA

### 4.1 âœ… O que estÃ¡ BEM
- TypeScript habilitado com interfaces bem definidas (`types/index.ts`).
- Separation of concerns entre Services/Hooks/Components/Lib.
- ErrorHandler robusto com ErrorCode enum e retry logic.
- Logger estruturado.
- XSS protection module.
- Validation module com Zod.
- `DataService` marcado como `DEPRECATION NOTICE` com facade para services especÃ­ficos.

### 4.2 ğŸ”´ INCONSISTÃŠNCIAS

#### FATAL-C1: `any` Em Todo Lugar â€” Tipagem Suja
**Exemplos:**
- `tenantService.ts`: `createTenant(tenant: any)`, `updateTenant(tenant: any)`, `createUser(userData: any)`, `updateUser(userData: any)` â€” TODOS `any`.
- `orderService.ts`: `_mapOrderToDB(order: any)`, `_mapOrderFromDB(data: any)`
- `supabase.ts`: `adminSupabase` castado como `as any` (linha 234)
- `AuthContext.tsx`: `authSubscriptionRef = useRef<any>(null)`, `systemNotifications = useState<any[]>([])`  
**Impacto:** O TypeScript existe mas perde 70% do valor. Bugs de tipagem passam despercebidos. Refactoring se torna arriscado.  
**Severidade:** ğŸ”´ CRÃTICA (para manutenibilidade)  
**CorreÃ§Ã£o:** Criar interfaces TypeScript para TODAS as entidades de banco de dados. Criar tipos para payloads de create/update.

#### FATAL-C2: LÃ³gica de NegÃ³cio Vazando para Componentes
**Exemplos:**
- `TechLogin.tsx` (14.8KB) â€” provavelmente faz fetch, auth, session management, navigation, tudo dentro do componente.
- `TechDashboard.tsx` (24.9KB) â€” provavelmente combina UI + data fetching + business logic.
- `OrderDetailsModal.tsx` (63.2KB) â€” monÃ³lito de UI + lÃ³gica.
- `App.tsx` linhas 62-63: LÃ³gica Master Login inline no JSX com callbacks complexos.  
**Severidade:** ğŸŸ  ALTA  
**CorreÃ§Ã£o:** Extrair lÃ³gica para hooks customizados (`useTechAuth`, `useTechDashboard`, `useOrderDetails`). Componentes devem ser apenas UI.

#### GRAVE-C3: DataService Facade Ã© um God Object
**Arquivo:** `src/services/dataService.ts` linhas 111-123  
**Problema:** Usa spread operator para merge de 12 services em um Ãºnico objeto. Isso:
- Cria colisÃµes de nome silenciosas (se dois services tÃªm o mesmo mÃ©todo, um sobrescreve o outro)
- Impossibilita tree-shaking
- Dificulta descoberta de tipos  
**Severidade:** ğŸŸ  ALTA  
**CorreÃ§Ã£o:** Parar de usar `DataService` como ponto de acesso. Importar services especÃ­ficos diretamente.

#### GRAVE-C4: Backup Files no RepositÃ³rio
**Arquivo:** `src/services/` contÃ©m:
- `authService.ts.backup`
- `authService.ts.bkp`
- `contractService.ts.bkp`
- `customerService.ts.bkp`
- `equipmentService.ts.bkp`
- `financialService.ts.bkp`
- `formService.ts.bak`
- `orderService.ts.bkp`
- `quoteService.ts.bkp`
- `stockService.ts.bkp`
- `technicianService.ts.bkp`  
**Problema:** 11 arquivos de backup no repositÃ³rio. Isso Ã© trabalho do Git, nÃ£o de `.bkp` files. Indica ausÃªncia de disciplina de versionamento.  
**Severidade:** ğŸŸ¡ MÃ‰DIA  
**CorreÃ§Ã£o:** Deletar todos os `.bkp`/`.backup`/`.bak`. Adicionar ao `.gitignore`.

#### GRAVE-C5: Import Circular Potencial em `tenantContext.ts`
**Arquivo:** `src/lib/tenantContext.ts` linha 177  
**Problema:** `import * as React from 'react'` estÃ¡ na ÃšLTIMA LINHA do arquivo, depois de jÃ¡ ter sido usado na linha 158. Em ambientes com ESM strict, isso pode falhar. Ã‰ um hoisting issue.  
**Severidade:** ğŸŸ¡ MÃ‰DIA  
**CorreÃ§Ã£o:** Mover o import para o topo do arquivo.

#### GRAVE-C6: 85 Arquivos de Migration Sem ConsolidaÃ§Ã£o
**Pasta:** `supabase/migrations/` â€” 85 arquivos  
**Problema:** Muitas migrations "fix" empilhadas (ex: `repair_tech_table`, `repair_tech_table_v2`, `repair_tech_table_v3`). Isso indica desenvolvimento sem planejamento de schema. Em produÃ§Ã£o com mÃºltiplos ambientes, isso se torna ingerenciÃ¡vel.  
**Severidade:** ğŸŸ¡ MÃ‰DIA  
**CorreÃ§Ã£o:** Consolidar em um **schema definitivo** + `seed.sql`. Manter migrations futuras limpas e sequenciais.

#### GRAVE-C7: `console.log` Massivo no CÃ³digo de ProduÃ§Ã£o
**Problema:** Apesar de `drop_console: true` no Terser (config Vite), o cÃ³digo estÃ¡ repleto de `console.log`, `console.warn` com emojis Debug. Em DEV isso Ã© aceitÃ¡vel, mas:
- Performance em mobile Ã© afetada por logging excessivo
- Strings ficam no bundle antes do terser processar
- `console.error` **NÃƒO** Ã© removido pelo Terser (corretamente, mas muitos erros sÃ£o informativos, nÃ£o erros reais)  
**Severidade:** ğŸŸ¡ MÃ‰DIA  
**CorreÃ§Ã£o:** Usar o `logger.ts` existente consistentemente. Remover `console.log` diretos.

#### GRAVE-C8: `publicSupabase` Criado Sem Necessidade
**Arquivo:** `src/lib/supabase.ts` linhas 237-243  
**Problema:** Um terceiro cliente Supabase Ã© criado sem sessÃ£o (`persistSession: false`). Isso Ã© usado para RPC pÃºblicas, mas poderia usar o `supabase` normal que jÃ¡ funciona para requests anon. Ter 3 clientes Ã© complexidade desnecessÃ¡ria.  
**Severidade:** ğŸŸ¡ MÃ‰DIA  
**CorreÃ§Ã£o:** Avaliar se o cliente anon default (`supabase`) jÃ¡ atende. Se RPCs pÃºblicas nÃ£o precisam de sessÃ£o, podem funcionar com o cliente padrÃ£o usando `.rpc()` sem autenticaÃ§Ã£o.

---

## ğŸ“Š ROADMAP DE ESTABILIZAÃ‡ÃƒO (Priorizado)

### ğŸ”´ FASE 1: BLOQUEANTES (Sem isso, o sistema NÃƒO pode ir para produÃ§Ã£o)
| # | Item | Severidade | EsforÃ§o | Arquivos |
|---|------|-----------|---------|----------|
| 1 | **Remover `adminSupabase` do frontend** â€” mover operaÃ§Ãµes admin para Edge Functions | ğŸ”´ğŸ”´ BLOQUEANTE | 3-5 dias | Todos os services + supabase.ts |
| 2 | **Corrigir RLS policies** â€” `users_select`, `users_delete` devem filtrar por tenant | ğŸ”´ğŸ”´ BLOQUEANTE | 2h | Migration SQL nova |
| 3 | **Remover fallback de URL params** para `getCurrentTenantId` em contextos autenticados | ğŸ”´ CRÃTICA | 4h | tenantContext.ts + todos services |
| 4 | **Centralizar `getCurrentTenantId`** â€” usar TenantContext singleton, deletar 8+ cÃ³pias | ğŸ”´ CRÃTICA | 1 dia | 8+ arquivos |
| 5 | **Implementar Service Worker funcional** com Workbox | ğŸ”´ğŸ”´ BLOQUEANTE | 3-5 dias | sw.js, vite.config.ts |
| 6 | **Corrigir race condition de Token Refresh** â€” remover refresh manual do AuthContext | ğŸ”´ CRÃTICA | 2h | AuthContext.tsx |
| 7 | **Substituir `adminSupabase` objeto hÃ­brido** por admin proxy separado | ğŸ”´ CRÃTICA | 1 dia | supabase.ts |

### ğŸŸ  FASE 2: GRAVES (Impedem escalabilidade e estabilidade)
| # | Item | Severidade | EsforÃ§o | Arquivos |
|---|------|-----------|---------|----------|
| 8 | Implementar Offline Mutation Queue com IndexedDB | ğŸŸ  ALTA | 3 dias | Novo mÃ³dulo |
| 9 | Prefixar cache keys do `useQuery` com tenant ID | ğŸŸ  ALTA | 2h | useQuery.ts |
| 10 | Adicionar `tenant_id` filter em todos os updates/deletes do frontend | ğŸŸ  ALTA | 4h | Todos os services |
| 11 | `deleteTenant` via Edge Function com transaction | ğŸŸ  ALTA | 1 dia | tenantService.ts + nova Edge Function |
| 12 | Fix handleFocus chamando validaÃ§Ã£o 2x | ğŸŸ  ALTA | 30min | AuthContext.tsx |
| 13 | Fix AbortController listener leak no custom fetch | ğŸŸ  ALTA | 1h | supabase.ts |
| 14 | Completar `manifest.json` com todos os tamanhos de Ã­cones | ğŸŸ  ALTA | 2h | manifest.json + assets |
| 15 | Quebrar `OrderDetailsModal.tsx` (63KB) em sub-componentes | ğŸŸ  ALTA | 2 dias | tech-pwa/ |

### ğŸŸ¡ FASE 3: MELHORIAS (Clean Code e DX)
| # | Item | Severidade | EsforÃ§o |
|---|------|-----------|---------|
| 16 | Eliminar todos os `any` â€” criar tipos strictos | ğŸŸ¡ MÃ‰DIA | 3 dias |
| 17 | Deletar 11 arquivos `.bkp` do repositÃ³rio | ğŸŸ¡ MÃ‰DIA | 15min |
| 18 | Consolidar 85 migrations em schema definitivo | ğŸŸ¡ MÃ‰DIA | 1 dia |
| 19 | Fix import do React no `tenantContext.ts` | ğŸŸ¡ MÃ‰DIA | 5min |
| 20 | Migrar `console.log` para `logger.ts` consistentemente | ğŸŸ¡ MÃ‰DIA | 4h |
| 21 | Extrair lÃ³gica de negÃ³cio dos componentes para hooks | ğŸŸ¡ MÃ‰DIA | 3 dias |
| 22 | Remover `DataService` facade â€” importar services diretamente | ğŸŸ¡ MÃ‰DIA | 2 dias |
| 23 | Habilitar e configurar audit log triggers em tabelas sensÃ­veis | ğŸŸ¡ MÃ‰DIA | 2h |

---

## ğŸ”’ VEREDICTO FINAL

### Para ser um SaaS Comercial Enterprise-Grade, o Nexus Line precisa:

| Requisito | Status Atual |
|-----------|-------------|
| Multi-tenancy infalÃ­vel | âŒ **FALHA** â€” RLS bypassed, policies com vazamento |
| ResiliÃªncia de sessÃ£o | âš ï¸ **PARCIAL** â€” Race conditions no token refresh |
| PWA funcional | âŒ **FALHA** â€” SW desabilitado, sem offline support |
| SeguranÃ§a de dados | âŒ **FALHA** â€” Admin proxy no frontend, URL param injection |
| Clean Code | âš ï¸ **PARCIAL** â€” `any` excessivo, duplicaÃ§Ã£o, God objects |
| Ready for APK | âŒ **FALHA** â€” Manifest incompleto, sem SW, sem state management |

### Estimativa Total de EstabilizaÃ§Ã£o:
- **Fase 1 (Bloqueantes):** ~2 semanas de trabalho focado
- **Fase 2 (Graves):** ~1.5 semanas
- **Fase 3 (Melhorias):** ~2 semanas

**Total: ~5-6 semanas para Enterprise-Grade.**

---

> *"O cÃ³digo que funciona em demo nÃ£o Ã© o cÃ³digo que sobrevive a 100 mil empresas simultÃ¢neas. A diferenÃ§a entre um MVP e um SaaS comercial estÃ¡ nos detalhes que ninguÃ©m vÃª â€” RLS policies, race conditions, offline resilience, e a disciplina de nÃ£o usar `any`."*
>
> â€” Auditoria L7, Nexus Line, 2026-02-18
